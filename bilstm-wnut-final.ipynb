{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "PAD = \"PAD\"\n",
    "UNK = \"UNK\"\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "\n",
    "def read_conll_file(dataset):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        words = example['tokens']\n",
    "        tags = example['ner_tags']\n",
    "        data.append((words, tags))\n",
    "    return data\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, word_vocab, tag_vocab):\n",
    "        self.data = data\n",
    "        self.word_vocab = word_vocab\n",
    "        self.tag_vocab = tag_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = self.data[idx]\n",
    "        word_indices = [self.word_vocab.get(word, self.word_vocab[UNK]) for word in words]\n",
    "        tag_indices = [self.tag_vocab[tag] for tag in tags]\n",
    "        return torch.tensor(word_indices, dtype=torch.long), torch.tensor(tag_indices, dtype=torch.long)\n",
    "\n",
    "def build_vocab(data):\n",
    "    word_vocab = defaultdict(lambda: len(word_vocab))\n",
    "    tag_vocab = defaultdict(lambda: len(tag_vocab))\n",
    "    word_vocab[PAD]\n",
    "    word_vocab[UNK]\n",
    "    for sentence, tags in data:\n",
    "        for word in sentence:\n",
    "            word_vocab[word]\n",
    "        for tag in tags:\n",
    "            tag_vocab[tag]\n",
    "    return dict(word_vocab), dict(tag_vocab)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    word_seqs, tag_seqs = zip(*batch)\n",
    "    word_seqs_padded = pad_sequence(word_seqs, batch_first=True, padding_value=0)\n",
    "    tag_seqs_padded = pad_sequence(tag_seqs, batch_first=True, padding_value=0)\n",
    "    return word_seqs_padded, tag_seqs_padded\n",
    "\n",
    "all_data = load_dataset('wnut_17')\n",
    "train_data = read_conll_file(all_data['train'])\n",
    "dev_data = read_conll_file(all_data['test'])\n",
    "\n",
    "word_vocab, tag_vocab = build_vocab(train_data)\n",
    "train_dataset = NERDataset(train_data, word_vocab, tag_vocab)\n",
    "dev_dataset = NERDataset(dev_data, word_vocab, tag_vocab)\n",
    "\n",
    "def create_subsets(dataset, fractions):\n",
    "    subsets = []\n",
    "    total_size = len(dataset)\n",
    "    indices = list(range(total_size))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    for fraction in fractions:\n",
    "        subset_size = int(fraction * total_size)\n",
    "        subset_indices = indices[:subset_size]\n",
    "        subsets.append(Subset(dataset, subset_indices))\n",
    "\n",
    "    return subsets\n",
    "\n",
    "fractions = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "train_subsets = create_subsets(train_dataset, fractions)\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim=DIM_EMBEDDING, hidden_dim=LSTM_HIDDEN):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        return tag_space\n",
    "\n",
    "def train_and_evaluate_on_subset(subset):\n",
    "    train_loader = DataLoader(subset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    model = SimpleLSTM(len(word_vocab), len(tag_vocab))\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for words, tags in train_loader:\n",
    "            model.zero_grad()\n",
    "            tag_scores = model(words)\n",
    "            loss = loss_function(tag_scores.view(-1, len(tag_vocab)), tags.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for words, tags in dev_loader:\n",
    "            tag_scores = model(words)\n",
    "            preds = torch.argmax(tag_scores, dim=2)\n",
    "            all_preds.extend(preds.view(-1).tolist())\n",
    "            all_labels.extend(tags.view(-1).tolist())\n",
    "\n",
    "    # Filter out padding tokens\n",
    "    filtered_preds = [p for p, l in zip(all_preds, all_labels) if l != -100]\n",
    "    filtered_labels = [l for l in all_labels if l != -100]\n",
    "\n",
    "    f1 = f1_score(filtered_labels, filtered_preds, average='macro')\n",
    "    return f1\n",
    "\n",
    "f1_scores = []\n",
    "for subset in train_subsets:\n",
    "    f1 = train_and_evaluate_on_subset(subset)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "\n",
    "plt.plot([fraction * 100 for fraction in fractions], f1_scores)\n",
    "plt.xlabel('Percentage of Data Used')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Learning Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Word vocab size: 20775\n",
      "Tag vocab size: 16\n",
      "Tag vocabulary: ['_PAD_', '<START>', '<STOP>', 'O', 'B-location', 'I-location', 'B-group', 'B-corporation', 'B-person', 'B-creative-work', 'B-product', 'I-person', 'I-creative-work', 'I-corporation', 'I-group', 'I-product']\n",
      "Class distribution:\n",
      "  _PAD_: 0 samples (weight: 0.000)\n",
      "  <START>: 0 samples (weight: 0.000)\n",
      "  <STOP>: 0 samples (weight: 0.000)\n",
      "  O: 59570 samples (weight: 0.081)\n",
      "  B-location: 548 samples (weight: 8.805)\n",
      "  I-location: 245 samples (weight: 19.695)\n",
      "  B-group: 264 samples (weight: 18.278)\n",
      "  B-corporation: 221 samples (weight: 21.834)\n",
      "  B-person: 660 samples (weight: 7.311)\n",
      "  B-creative-work: 140 samples (weight: 34.467)\n",
      "  B-product: 142 samples (weight: 33.982)\n",
      "  I-person: 335 samples (weight: 14.404)\n",
      "  I-creative-work: 206 samples (weight: 23.424)\n",
      "  I-corporation: 46 samples (weight: 104.900)\n",
      "  I-group: 150 samples (weight: 32.169)\n",
      "  I-product: 203 samples (weight: 23.770)\n",
      "\n",
      "=== Starting Enhanced BiLSTM-CRF Experiments ===\n",
      "\n",
      "Training Enhanced BiLSTM-CRF on 20% of data (678 samples)...\n",
      "Subset: 20%, Epoch 1/10\n",
      "  Train Loss: -9980.1041\n",
      "  Dev Accuracy: 0.8620, Macro F1: 0.0674, Entity F1: 0.0012\n",
      "Subset: 20%, Epoch 2/10\n",
      "  Train Loss: -9992.8912\n",
      "  Dev Accuracy: 0.8567, Macro F1: 0.0662, Entity F1: 0.0061\n",
      "Subset: 20%, Epoch 3/10\n",
      "  Train Loss: -9995.9975\n",
      "  Dev Accuracy: 0.8594, Macro F1: 0.0646, Entity F1: 0.0036\n",
      "Subset: 20%, Epoch 4/10\n",
      "  Train Loss: -9998.2799\n",
      "  Dev Accuracy: 0.8702, Macro F1: 0.0678, Entity F1: 0.0070\n",
      "Subset: 20%, Epoch 5/10\n",
      "  Train Loss: -10000.1039\n",
      "  Dev Accuracy: 0.8823, Macro F1: 0.0718, Entity F1: 0.0118\n",
      "Subset: 20%, Epoch 6/10\n",
      "  Train Loss: -10001.5601\n",
      "  Dev Accuracy: 0.9049, Macro F1: 0.0690, Entity F1: 0.0071\n",
      "Subset: 20%, Epoch 7/10\n",
      "  Train Loss: -10002.9376\n",
      "  Dev Accuracy: 0.8957, Macro F1: 0.0705, Entity F1: 0.0103\n",
      "Subset: 20%, Epoch 8/10\n",
      "  Train Loss: -10003.6704\n",
      "  Dev Accuracy: 0.9030, Macro F1: 0.0787, Entity F1: 0.0146\n",
      "Subset: 20%, Epoch 9/10\n",
      "  Train Loss: -10005.2075\n",
      "  Dev Accuracy: 0.9137, Macro F1: 0.0716, Entity F1: 0.0048\n",
      "\n",
      "=== DETAILED EVALUATION ===\n",
      "Prediction distribution:\n",
      "  _PAD_           - Predicted:      2, True:      0\n",
      "  <START>         - Predicted:    262, True:      0\n",
      "  <STOP>          - Predicted:      0, True:      0\n",
      "  O               - Predicted:  15323, True:  14483\n",
      "  B-location      - Predicted:     14, True:     74\n",
      "  I-location      - Predicted:      9, True:     33\n",
      "  B-group         - Predicted:     15, True:     39\n",
      "  B-corporation   - Predicted:      3, True:     34\n",
      "  B-person        - Predicted:     56, True:    470\n",
      "  B-creative-work - Predicted:      7, True:    105\n",
      "  B-product       - Predicted:     12, True:    114\n",
      "  I-person        - Predicted:      7, True:    117\n",
      "  I-creative-work - Predicted:     15, True:    133\n",
      "  I-corporation   - Predicted:      0, True:     12\n",
      "  I-group         - Predicted:      5, True:     25\n",
      "  I-product       - Predicted:      3, True:     94\n",
      "\n",
      "Overall Metrics:\n",
      "  Accuracy: 0.9014\n",
      "  Macro F1: 0.0741\n",
      "  Macro Precision: 0.1181\n",
      "  Macro Recall: 0.0714\n",
      "\n",
      "Entity-only Metrics (excluding O):\n",
      "  Entity F1: 0.0147\n",
      "  Entity Precision: 0.1716\n",
      "\n",
      "Classification Report (Entities Only):\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "     B-location       0.00      0.00      0.00        74\n",
      "     I-location       0.00      0.00      0.00        33\n",
      "        B-group       0.25      0.03      0.05        39\n",
      "  B-corporation       0.50      0.03      0.06        34\n",
      "       B-person       0.64      0.02      0.04       470\n",
      "B-creative-work       0.67      0.02      0.04       105\n",
      "      B-product       0.00      0.00      0.00       114\n",
      "       I-person       0.00      0.00      0.00       117\n",
      "I-creative-work       0.00      0.00      0.00       133\n",
      "  I-corporation       0.00      0.00      0.00        12\n",
      "        I-group       0.00      0.00      0.00        25\n",
      "      I-product       0.00      0.00      0.00        94\n",
      "\n",
      "      micro avg       0.31      0.01      0.02      1250\n",
      "      macro avg       0.17      0.01      0.01      1250\n",
      "   weighted avg       0.32      0.01      0.02      1250\n",
      "\n",
      "Subset: 20%, Epoch 10/10\n",
      "  Train Loss: -10006.3809\n",
      "  Dev Accuracy: 0.9014, Macro F1: 0.0741, Entity F1: 0.0147\n",
      "\n",
      "Training Enhanced BiLSTM-CRF on 40% of data (1357 samples)...\n",
      "Subset: 40%, Epoch 1/10\n",
      "  Train Loss: -9971.5846\n",
      "  Dev Accuracy: 0.8608, Macro F1: 0.0673, Entity F1: 0.0072\n",
      "Subset: 40%, Epoch 2/10\n",
      "  Train Loss: -9990.6622\n",
      "  Dev Accuracy: 0.8987, Macro F1: 0.0820, Entity F1: 0.0260\n",
      "Subset: 40%, Epoch 3/10\n",
      "  Train Loss: -9995.2326\n",
      "  Dev Accuracy: 0.8963, Macro F1: 0.0648, Entity F1: 0.0077\n",
      "Subset: 40%, Epoch 4/10\n",
      "  Train Loss: -9998.6172\n",
      "  Dev Accuracy: 0.9071, Macro F1: 0.0887, Entity F1: 0.0283\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import defaultdict, Counter\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#Constants\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TAG_STR = \"_PAD_\"\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "\n",
    "DIM_EMBEDDING = 100\n",
    "LSTM_HIDDEN = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "#Focal Loss Implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for addressing class imbalance.\"\"\"\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, ignore_index=-100):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, ignore_index=self.ignore_index, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "#Enhanced CRF Implementation\n",
    "class CRF(nn.Module):\n",
    "    \"\"\"Conditional Random Field layer with class weighting support.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_tags, start_tag_idx, end_tag_idx, pad_tag_idx, class_weights=None):\n",
    "        super(CRF, self).__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.start_tag_idx = start_tag_idx\n",
    "        self.end_tag_idx = end_tag_idx\n",
    "        self.pad_tag_idx = pad_tag_idx\n",
    "        \n",
    "        if class_weights is not None:\n",
    "            self.register_buffer('class_weights', torch.FloatTensor(class_weights))\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "        \n",
    "        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))\n",
    "        \n",
    "        self.transitions.data[start_tag_idx, :] = -10000\n",
    "        self.transitions.data[:, end_tag_idx] = -10000\n",
    "        \n",
    "    def _compute_partition_function(self, emissions, mask):\n",
    "        \"\"\"Compute the partition function using forward algorithm.\"\"\"\n",
    "        batch_size, seq_length, num_tags = emissions.shape\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            emissions = emissions * self.class_weights.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        alpha = torch.full((batch_size, num_tags), -10000.0, device=emissions.device)\n",
    "        alpha[:, self.start_tag_idx] = 0.0\n",
    "        \n",
    "        for i in range(seq_length):\n",
    "            alpha_broadcast = alpha.unsqueeze(2).expand(batch_size, num_tags, num_tags)\n",
    "            transition_scores = alpha_broadcast + self.transitions.unsqueeze(0)\n",
    "            emission_scores = emissions[:, i, :]\n",
    "            new_alpha = torch.logsumexp(transition_scores, dim=1) + emission_scores\n",
    "            alpha = torch.where(mask[:, i].unsqueeze(1), new_alpha, alpha)\n",
    "        \n",
    "        final_alpha = alpha + self.transitions[self.end_tag_idx].unsqueeze(0)\n",
    "        return torch.logsumexp(final_alpha, dim=1)\n",
    "    \n",
    "    def _compute_score(self, emissions, tags, mask):\n",
    "        \"\"\"Compute the score of the given tag sequence.\"\"\"\n",
    "        batch_size, seq_length = tags.shape\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            emissions = emissions * self.class_weights.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        tags_with_start_end = torch.full((batch_size, seq_length + 2), self.pad_tag_idx, \n",
    "                                         dtype=torch.long, device=tags.device)\n",
    "        tags_with_start_end[:, 0] = self.start_tag_idx\n",
    "        tags_with_start_end[:, 1:-1] = tags\n",
    "        tags_with_start_end[:, -1] = self.end_tag_idx\n",
    "        \n",
    "        mask_with_start_end = torch.ones((batch_size, seq_length + 2), dtype=torch.bool, device=mask.device)\n",
    "        mask_with_start_end[:, 1:-1] = mask\n",
    "        \n",
    "        score = torch.zeros(batch_size, device=emissions.device)\n",
    "        \n",
    "        for i in range(seq_length):\n",
    "            valid_mask = mask[:, i]\n",
    "            if valid_mask.any():\n",
    "                score[valid_mask] += emissions[valid_mask, i, tags[valid_mask, i]]\n",
    "        \n",
    "        for i in range(seq_length + 1):\n",
    "            valid_mask = mask_with_start_end[:, i+1]\n",
    "            if valid_mask.any():\n",
    "                score[valid_mask] += self.transitions[\n",
    "                    tags_with_start_end[valid_mask, i+1], \n",
    "                    tags_with_start_end[valid_mask, i]\n",
    "                ]\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def forward(self, emissions, tags, mask):\n",
    "        \"\"\"Compute the negative log likelihood.\"\"\"\n",
    "        partition = self._compute_partition_function(emissions, mask)\n",
    "        score = self._compute_score(emissions, tags, mask)\n",
    "        return (partition - score).mean()\n",
    "    \n",
    "    def decode(self, emissions, mask):\n",
    "        \"\"\"Find the most likely tag sequence using Viterbi algorithm.\"\"\"\n",
    "        batch_size, seq_length, num_tags = emissions.shape\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            emissions = emissions * self.class_weights.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        viterbi_score = torch.full((batch_size, num_tags), -10000.0, device=emissions.device)\n",
    "        viterbi_score[:, self.start_tag_idx] = 0.0\n",
    "        viterbi_path = []\n",
    "        \n",
    "        for i in range(seq_length):\n",
    "            broadcast_score = viterbi_score.unsqueeze(2).expand(batch_size, num_tags, num_tags)\n",
    "            transition_scores = broadcast_score + self.transitions.unsqueeze(0)\n",
    "            best_score, best_path = torch.max(transition_scores, dim=1)\n",
    "            new_score = best_score + emissions[:, i, :]\n",
    "            viterbi_score = torch.where(mask[:, i].unsqueeze(1), new_score, viterbi_score)\n",
    "            viterbi_path.append(best_path)\n",
    "        \n",
    "        final_score = viterbi_score + self.transitions[self.end_tag_idx].unsqueeze(0)\n",
    "        best_final_score, last_tag = torch.max(final_score, dim=1)\n",
    "        \n",
    "        best_path = []\n",
    "        for i in range(batch_size):\n",
    "            path = [last_tag[i].item()]\n",
    "            \n",
    "            for j in range(seq_length - 1, -1, -1):\n",
    "                if mask[i, j]:\n",
    "                    path.append(viterbi_path[j][i, path[-1]].item())\n",
    "            path.reverse()\n",
    "            best_path.append(path[1:])\n",
    "        \n",
    "        return best_path\n",
    "\n",
    "#Data Processing Functions\n",
    "def process_dataset_examples(dataset_split, tag_names_list):\n",
    "    \"\"\"Processes examples from a Hugging Face dataset split.\"\"\"\n",
    "    data = []\n",
    "    for example in dataset_split:\n",
    "        words = example['tokens']\n",
    "        str_tags = [tag_names_list[t] for t in example['ner_tags']]\n",
    "        data.append((words, str_tags))\n",
    "    return data\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, word_vocab, tag_vocab):\n",
    "        self.data = data\n",
    "        self.word_vocab = word_vocab\n",
    "        self.tag_vocab = tag_vocab\n",
    "        self.unk_word_idx = self.word_vocab[UNK_TOKEN]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags_str = self.data[idx]\n",
    "        word_indices = [self.word_vocab.get(word, self.unk_word_idx) for word in words]\n",
    "        tag_indices = [self.tag_vocab[tag] for tag in tags_str]\n",
    "        return torch.tensor(word_indices, dtype=torch.long), torch.tensor(tag_indices, dtype=torch.long)\n",
    "\n",
    "def build_vocab(data, tag_names_from_hf):\n",
    "    word_vocab = defaultdict(lambda: len(word_vocab))\n",
    "    tag_vocab_str_to_int = defaultdict(lambda: len(tag_vocab_str_to_int))\n",
    "\n",
    "    word_vocab[PAD_TOKEN]\n",
    "    word_vocab[UNK_TOKEN]\n",
    "\n",
    "    tag_vocab_str_to_int[PAD_TAG_STR]\n",
    "    tag_vocab_str_to_int[START_TAG]\n",
    "    tag_vocab_str_to_int[STOP_TAG]\n",
    "\n",
    "    for sentence, tags_str_list in data:\n",
    "        for word in sentence:\n",
    "            word_vocab[word]\n",
    "        for tag_str in tags_str_list:\n",
    "            tag_vocab_str_to_int[tag_str]\n",
    "\n",
    "    for tag_name in tag_names_from_hf:\n",
    "        tag_vocab_str_to_int[tag_name]\n",
    "\n",
    "    return dict(word_vocab), dict(tag_vocab_str_to_int)\n",
    "\n",
    "def collate_fn(batch, word_pad_idx, tag_pad_idx):\n",
    "    word_seqs, tag_seqs = zip(*batch)\n",
    "    word_seqs_padded = pad_sequence(word_seqs, batch_first=True, padding_value=word_pad_idx)\n",
    "    tag_seqs_padded = pad_sequence(tag_seqs, batch_first=True, padding_value=tag_pad_idx)\n",
    "    return word_seqs_padded, tag_seqs_padded\n",
    "\n",
    "def compute_class_weights_from_data(train_data, tag_vocab):\n",
    "    \"\"\"Compute class weights to handle imbalance.\"\"\"\n",
    "    all_tags = []\n",
    "    for _, tags in train_data:\n",
    "        all_tags.extend([tag_vocab[tag] for tag in tags])\n",
    "    \n",
    "    observed_tag_indices = list(set(all_tags))\n",
    "    \n",
    "    if len(all_tags) == 0: \n",
    "        print(\"No tags in training data to compute weights. Returning uniform weights.\")\n",
    "        return np.ones(len(tag_vocab))\n",
    "\n",
    "    class_weights_raw = compute_class_weight('balanced', classes=np.array(sorted(observed_tag_indices)), y=all_tags)\n",
    "    \n",
    "    full_class_weights = np.ones(len(tag_vocab), dtype=np.float32)\n",
    "    for i, tag_idx in enumerate(sorted(observed_tag_indices)):\n",
    "        full_class_weights[tag_idx] = class_weights_raw[i]\n",
    "\n",
    "    if tag_vocab[PAD_TAG_STR] < len(full_class_weights):\n",
    "        full_class_weights[tag_vocab[PAD_TAG_STR]] = 0.0\n",
    "    if tag_vocab[START_TAG] < len(full_class_weights):\n",
    "        full_class_weights[tag_vocab[START_TAG]] = 0.0\n",
    "    if tag_vocab[STOP_TAG] < len(full_class_weights):\n",
    "        full_class_weights[tag_vocab[STOP_TAG]] = 0.0\n",
    "\n",
    "    print(\"Class distribution:\")\n",
    "    tag_counter = Counter(all_tags)\n",
    "    for tag_name, tag_idx in sorted(tag_vocab.items(), key=lambda x: x[1]):\n",
    "        count = tag_counter.get(tag_idx, 0)\n",
    "        weight = full_class_weights[tag_idx]\n",
    "        print(f\"  {tag_name}: {count} samples (weight: {weight:.3f})\")\n",
    "    \n",
    "    return full_class_weights\n",
    "\n",
    "#Model Definition\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, word_pad_idx, \n",
    "                 start_tag_idx, end_tag_idx, pad_tag_idx, class_weights=None):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=word_pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.crf = CRF(tagset_size, start_tag_idx, end_tag_idx, pad_tag_idx, class_weights)\n",
    "\n",
    "    def forward(self, sentence_batch, tags_batch=None, mask=None):\n",
    "        embeds = self.embedding(sentence_batch)\n",
    "        embeds = self.dropout(embeds)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        \n",
    "        if tags_batch is not None:\n",
    "            return self.crf(emissions, tags_batch, mask)\n",
    "        else:\n",
    "            return self.crf.decode(emissions, mask)\n",
    "\n",
    "#Enhanced Evaluation\n",
    "def create_mask(padded_sequences, pad_idx):\n",
    "    \"\"\"Create mask for padded sequences.\"\"\"\n",
    "    return (padded_sequences != pad_idx)\n",
    "\n",
    "def get_entity_labels(tag_vocab, exclude_tags={'_PAD_', '<START>', '<STOP>', 'O'}):\n",
    "    \"\"\"Get indices of entity tags (excluding O, PAD, START, STOP).\"\"\"\n",
    "    entity_indices = []\n",
    "    entity_names = []\n",
    "    for tag_name, tag_idx in tag_vocab.items():\n",
    "        if tag_name not in exclude_tags:\n",
    "            entity_indices.append(tag_idx)\n",
    "            entity_names.append(tag_name)\n",
    "    return entity_indices, entity_names\n",
    "\n",
    "def evaluate_model_enhanced(model, data_loader, tag_pad_idx, device, tag_vocab, verbose=False):\n",
    "    \"\"\"Enhanced evaluation with detailed metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds_flat = []\n",
    "    all_labels_flat = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentences, tags in data_loader:\n",
    "            sentences, tags = sentences.to(device), tags.to(device)\n",
    "            mask = create_mask(tags, tag_pad_idx)\n",
    "            \n",
    "            pred_sequences = model(sentences, mask=mask)\n",
    "            \n",
    "            for i, (pred_seq, true_seq, seq_mask) in enumerate(zip(pred_sequences, tags, mask)):\n",
    "                valid_length = seq_mask.sum().item()\n",
    "\n",
    "                pred_seq_valid = pred_seq[:valid_length]\n",
    "                true_seq_valid = true_seq[:valid_length].cpu().numpy()\n",
    "                \n",
    "                all_preds_flat.extend(pred_seq_valid)\n",
    "                all_labels_flat.extend(true_seq_valid)\n",
    "    \n",
    "    if not all_labels_flat:\n",
    "        print(\"No valid labels found for evaluation.\")\n",
    "        return 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    \n",
    "    # Overall metrics\n",
    "    accuracy = accuracy_score(all_labels_flat, all_preds_flat)\n",
    "    precision_macro = precision_score(all_labels_flat, all_preds_flat, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(all_labels_flat, all_preds_flat, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(all_labels_flat, all_preds_flat, average='macro', zero_division=0)\n",
    "    \n",
    "    # Entity-only metrics (excluding O tag)\n",
    "    entity_indices, entity_names = get_entity_labels(tag_vocab)\n",
    "    \n",
    "    filtered_preds_for_entity = []\n",
    "    filtered_labels_for_entity = []\n",
    "    for p, l in zip(all_preds_flat, all_labels_flat):\n",
    "        if l in entity_indices: \n",
    "            filtered_labels_for_entity.append(l)\n",
    "            filtered_preds_for_entity.append(p)\n",
    "    \n",
    "    f1_entity = 0.0\n",
    "    precision_entity = 0.0\n",
    "    \n",
    "    if filtered_labels_for_entity: \n",
    "        f1_entity = f1_score(filtered_labels_for_entity, filtered_preds_for_entity, \n",
    "                             labels=entity_indices, average='macro', zero_division=0)\n",
    "        precision_entity = precision_score(filtered_labels_for_entity, filtered_preds_for_entity, \n",
    "                                          labels=entity_indices, average='macro', zero_division=0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n=== DETAILED EVALUATION ===\")\n",
    "        pred_counter = Counter(all_preds_flat)\n",
    "        true_counter = Counter(all_labels_flat)\n",
    "        \n",
    "        print(\"Prediction distribution:\")\n",
    "        for tag_name, tag_idx in sorted(tag_vocab.items(), key=lambda x: x[1]):\n",
    "            pred_count = pred_counter.get(tag_idx, 0)\n",
    "            true_count = true_counter.get(tag_idx, 0)\n",
    "            print(f\"  {tag_name:15} - Predicted: {pred_count:6}, True: {true_count:6}\")\n",
    "        \n",
    "        print(f\"\\nOverall Metrics:\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Macro F1: {f1_macro:.4f}\")\n",
    "        print(f\"  Macro Precision: {precision_macro:.4f}\")\n",
    "        print(f\"  Macro Recall: {recall_macro:.4f}\")\n",
    "        \n",
    "        print(f\"\\nEntity-only Metrics (excluding O):\")\n",
    "        print(f\"  Entity F1: {f1_entity:.4f}\")\n",
    "        print(f\"  Entity Precision: {precision_entity:.4f}\")\n",
    "        \n",
    "        if entity_indices and filtered_labels_for_entity:\n",
    "            idx_to_tag_name = {v: k for k, v in tag_vocab.items()}\n",
    "            target_names_for_report = [idx_to_tag_name[idx] for idx in entity_indices]\n",
    "            print(f\"\\nClassification Report (Entities Only):\")\n",
    "            print(classification_report(filtered_labels_for_entity, filtered_preds_for_entity, \n",
    "                                        labels=entity_indices, target_names=target_names_for_report, \n",
    "                                        zero_division=0))\n",
    "    \n",
    "    return accuracy, precision_macro, recall_macro, f1_macro, f1_entity, precision_entity\n",
    "\n",
    "def train_model_enhanced(model, train_loader, dev_loader, optimizer, epochs, tag_pad_idx, device, tag_vocab, subset_fraction):\n",
    "    \"\"\"Enhanced training with better monitoring.\"\"\"\n",
    "    best_entity_f1 = -1\n",
    "    metrics_history = {'accuracy': [], 'f1_macro': [], 'f1_entity': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for sentences, tags in train_loader:\n",
    "            sentences, tags = sentences.to(device), tags.to(device)\n",
    "            mask = create_mask(tags, tag_pad_idx)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss = model(sentences, tags, mask)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_train_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "        \n",
    "        #Evaluate\n",
    "        accuracy, precision_macro, recall_macro, f1_macro, f1_entity, precision_entity = evaluate_model_enhanced(\n",
    "            model, dev_loader, tag_pad_idx, device, tag_vocab, verbose=(epoch == epochs-1)\n",
    "        )\n",
    "        \n",
    "        print(f\"Subset: {subset_fraction*100:.0f}%, Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Dev Accuracy: {accuracy:.4f}, Macro F1: {f1_macro:.4f}, Entity F1: {f1_entity:.4f}\")\n",
    "\n",
    "        metrics_history['accuracy'].append(accuracy)\n",
    "        metrics_history['f1_macro'].append(f1_macro)\n",
    "        metrics_history['f1_entity'].append(f1_entity)\n",
    "        \n",
    "        if f1_entity > best_entity_f1:\n",
    "            best_entity_f1 = f1_entity\n",
    "\n",
    "    return accuracy, precision_macro, recall_macro, f1_macro, f1_entity, model\n",
    "\n",
    "#Robustness Testing Functions\n",
    "def introduce_typos(tokens, typo_rate=0.1):\n",
    "    \"\"\"Introduces typos into a list of tokens.\"\"\"\n",
    "    noisy_tokens = []\n",
    "    for token in tokens:\n",
    "        if random.random() < typo_rate and len(token) > 1:\n",
    "            idx_to_change = random.randint(0, len(token) - 1)\n",
    "            char_code = ord(token[idx_to_change])\n",
    "            try:\n",
    "                if 'a' <= token[idx_to_change] <= 'z':\n",
    "                    noisy_char_code = char_code + random.choice([-1, 1])\n",
    "                    if not ('a' <= chr(noisy_char_code) <= 'z'):\n",
    "                        noisy_char_code = random.randint(ord('a'), ord('z'))\n",
    "                elif 'A' <= token[idx_to_change] <= 'Z':\n",
    "                    noisy_char_code = char_code + random.choice([-1, 1])\n",
    "                    if not ('A' <= chr(noisy_char_code) <= 'Z'):\n",
    "                        noisy_char_code = random.randint(ord('A'), ord('Z'))\n",
    "                elif '0' <= token[idx_to_change] <= '9':\n",
    "                    noisy_char_code = char_code + random.choice([-1, 1])\n",
    "                    if not ('0' <= chr(noisy_char_code) <= '9'):\n",
    "                        noisy_char_code = random.randint(ord('0'), ord('9'))\n",
    "                else: \n",
    "                    noisy_char_code = random.randint(ord('a'), ord('z'))\n",
    "\n",
    "                noisy_char = chr(noisy_char_code)\n",
    "                noisy_token = list(token)\n",
    "                noisy_token[idx_to_change] = noisy_char\n",
    "                noisy_tokens.append(\"\".join(noisy_token))\n",
    "            except ValueError:\n",
    "                noisy_tokens.append(token)\n",
    "        else:\n",
    "            noisy_tokens.append(token)\n",
    "    return noisy_tokens\n",
    "\n",
    "def introduce_unks(tokens, word_vocab, unk_rate=0.1):\n",
    "    \"\"\"Replaces some tokens with the UNK_TOKEN string.\"\"\"\n",
    "    noisy_tokens = []\n",
    "    for token in tokens:\n",
    "        if random.random() < unk_rate and token in word_vocab and word_vocab[token] != word_vocab[UNK_TOKEN]:\n",
    "            noisy_tokens.append(UNK_TOKEN)\n",
    "        else:\n",
    "            noisy_tokens.append(token)\n",
    "    return noisy_tokens\n",
    "\n",
    "#Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load WNUT17\n",
    "    wnut_dataset = load_dataset('wnut_17')\n",
    "    tag_names_list = wnut_dataset['train'].features['ner_tags'].feature.names\n",
    "\n",
    "    train_data_processed = process_dataset_examples(wnut_dataset['train'], tag_names_list)\n",
    "    dev_data_processed = process_dataset_examples(wnut_dataset['validation'], tag_names_list)\n",
    "    test_data_processed = process_dataset_examples(wnut_dataset['test'], tag_names_list)\n",
    "\n",
    "    all_data_for_vocab = train_data_processed + dev_data_processed + test_data_processed\n",
    "    word_vocab, tag_vocab = build_vocab(all_data_for_vocab, tag_names_list)\n",
    "    \n",
    "    word_pad_idx = word_vocab[PAD_TOKEN]\n",
    "    tag_pad_idx = tag_vocab[PAD_TAG_STR]\n",
    "    start_tag_idx = tag_vocab[START_TAG]\n",
    "    end_tag_idx = tag_vocab[STOP_TAG]\n",
    "    num_tags = len(tag_vocab)\n",
    "\n",
    "    print(f\"Word vocab size: {len(word_vocab)}\")\n",
    "    print(f\"Tag vocab size: {num_tags}\")\n",
    "    print(f\"Tag vocabulary: {list(tag_vocab.keys())}\")\n",
    "\n",
    "    class_weights = compute_class_weights_from_data(train_data_processed, tag_vocab)\n",
    "    \n",
    "    full_train_dataset = NERDataset(train_data_processed, word_vocab, tag_vocab)\n",
    "    dev_dataset_torch = NERDataset(dev_data_processed, word_vocab, tag_vocab)\n",
    "    \n",
    "    dev_loader = DataLoader(dev_dataset_torch, batch_size=BATCH_SIZE, \n",
    "                            collate_fn=lambda b: collate_fn(b, word_pad_idx, tag_pad_idx))\n",
    "\n",
    "    print(\"\\n=== Starting Enhanced BiLSTM-CRF Experiments ===\")\n",
    "    fractions = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    data_efficiency_results = defaultdict(list)\n",
    "    \n",
    "    trained_model_full_data = None\n",
    "\n",
    "    for fraction in fractions:\n",
    "        subset_size = int(fraction * len(full_train_dataset))\n",
    "        indices = list(range(len(full_train_dataset)))\n",
    "        random.shuffle(indices)\n",
    "        train_subset_torch = Subset(full_train_dataset, indices[:subset_size])\n",
    "        \n",
    "        train_loader_subset = DataLoader(train_subset_torch, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                         collate_fn=lambda b: collate_fn(b, word_pad_idx, tag_pad_idx))\n",
    "\n",
    "        model = BiLSTM_CRF(len(word_vocab), num_tags, DIM_EMBEDDING, LSTM_HIDDEN, word_pad_idx,\n",
    "                           start_tag_idx, end_tag_idx, tag_pad_idx, class_weights).to(DEVICE)\n",
    "        \n",
    "        optimizer = optim.Adam([\n",
    "            {'params': model.embedding.parameters(), 'lr': LEARNING_RATE * 0.1},\n",
    "            {'params': model.lstm.parameters(), 'lr': LEARNING_RATE},\n",
    "            {'params': model.hidden2tag.parameters(), 'lr': LEARNING_RATE},\n",
    "            {'params': model.crf.parameters(), 'lr': LEARNING_RATE}\n",
    "        ])\n",
    "\n",
    "        print(f\"\\nTraining Enhanced BiLSTM-CRF on {fraction*100:.0f}% of data ({len(train_subset_torch)} samples)...\")\n",
    "        accuracy, precision, recall, f1_macro, f1_entity, trained_model_current = train_model_enhanced(\n",
    "            model, train_loader_subset, dev_loader, optimizer, EPOCHS, tag_pad_idx, DEVICE, tag_vocab, fraction\n",
    "        )\n",
    "        \n",
    "        data_efficiency_results['fraction'].append(fraction * 100)\n",
    "        data_efficiency_results['accuracy'].append(accuracy)\n",
    "        data_efficiency_results['precision'].append(precision)\n",
    "        data_efficiency_results['recall'].append(recall)\n",
    "        data_efficiency_results['f1_macro'].append(f1_macro)\n",
    "        data_efficiency_results['f1_entity'].append(f1_entity)\n",
    "\n",
    "        if fraction == 1.0:\n",
    "            trained_model_full_data = trained_model_current\n",
    "\n",
    "    # Plotting Results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(data_efficiency_results['fraction'], data_efficiency_results['accuracy'], 'o-', label='Accuracy')\n",
    "    plt.plot(data_efficiency_results['fraction'], data_efficiency_results['f1_macro'], 's-', label='Macro F1')\n",
    "    plt.plot(data_efficiency_results['fraction'], data_efficiency_results['f1_entity'], '^-', label='Entity F1')\n",
    "    plt.xlabel('Training Data (%)')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Enhanced BiLSTM-CRF: Data Efficiency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(data_efficiency_results['fraction'], data_efficiency_results['precision'], 'o-', label='Precision')\n",
    "    plt.plot(data_efficiency_results['fraction'], data_efficiency_results['recall'], 's-', label='Recall')\n",
    "    plt.xlabel('Training Data (%)')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Precision vs Recall')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"enhanced_bilstm_crf_results.png\", dpi=300, bbox_inches='tight')\n",
    "    print(\"Saved enhanced results plot to enhanced_bilstm_crf_results.png\")\n",
    "\n",
    "    # Robustness Testing\n",
    "    if trained_model_full_data:\n",
    "        print(\"\\n=== Robustness Testing ===\")\n",
    "        \n",
    "        # Clean evaluation\n",
    "        print(\"Clean Dev Set:\")\n",
    "        evaluate_model_enhanced(trained_model_full_data, dev_loader, tag_pad_idx, DEVICE, tag_vocab, verbose=True)\n",
    "        \n",
    "        # Typos evaluation\n",
    "        print(f\"\\n--- With Typos (10%) ---\")\n",
    "        typo_rate = 0.1\n",
    "        dev_data_typos = [(introduce_typos(words, typo_rate), tags) for words, tags in dev_data_processed]\n",
    "        dev_dataset_typos = NERDataset(dev_data_typos, word_vocab, tag_vocab)\n",
    "        dev_loader_typos = DataLoader(dev_dataset_typos, batch_size=BATCH_SIZE,\n",
    "                                      collate_fn=lambda b: collate_fn(b, word_pad_idx, tag_pad_idx))\n",
    "        evaluate_model_enhanced(trained_model_full_data, dev_loader_typos, tag_pad_idx, DEVICE, tag_vocab, verbose=True)\n",
    "        \n",
    "        # UNK evaluation\n",
    "        print(f\"\\n--- With UNKs (10%) ---\")\n",
    "        unk_rate = 0.1\n",
    "        dev_data_unks = [(introduce_unks(words, word_vocab, unk_rate), tags) for words, tags in dev_data_processed]\n",
    "        dev_dataset_unks = NERDataset(dev_data_unks, word_vocab, tag_vocab)\n",
    "        dev_loader_unks = DataLoader(dev_dataset_unks, batch_size=BATCH_SIZE,\n",
    "                                     collate_fn=lambda b: collate_fn(b, word_pad_idx, tag_pad_idx))\n",
    "        evaluate_model_enhanced(trained_model_full_data, dev_loader_unks, tag_pad_idx, DEVICE, tag_vocab, verbose=True)\n",
    "\n",
    "    print(\"\\n=== Enhanced BiLSTM-CRF Experiments Complete ===\")\n",
    "    print(\"Key improvements implemented:\")\n",
    "    print(\"- Class weighting to handle imbalance\")\n",
    "    print(\"- Separate entity-focused F1 metrics\")\n",
    "    print(\"- Enhanced evaluation with detailed breakdowns\")\n",
    "    print(\"- Gradient clipping and dropout for better training\")\n",
    "    print(\"- Different learning rates for different components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
